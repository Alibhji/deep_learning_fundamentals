{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Transformers: Encoders, Decoders, and Encoder–Decoder\n",
        "\n",
        "This notebook introduces transformer architectures for text: encoder-only (BERT-style), decoder-only (GPT-style), and encoder–decoder (T5/BART-style). It provides intuition, key equations, and a minimal runnable demo.\n",
        "\n",
        "## Learning objectives\n",
        "- Understand self-attention and multi-head attention\n",
        "- Compare encoder-only, decoder-only, and encoder–decoder designs\n",
        "- Run a tiny forward pass to solidify concepts\n",
        "\n",
        "## Outline\n",
        "1. Recap: Scaled dot-product attention\n",
        "2. Encoder-only: contextual understanding (BERT family)\n",
        "3. Decoder-only: autoregressive generation (GPT family)\n",
        "4. Encoder–decoder: seq2seq (T5/BART)\n",
        "5. Minimal demo\n",
        "\n",
        "## References (Papers)\n",
        "- Vaswani et al., 2017 — \"Attention Is All You Need\" (arXiv:1706.03762)\n",
        "- Devlin et al., 2018 — \"BERT: Pre-training of Deep Bidirectional Transformers\" (arXiv:1810.04805)\n",
        "- Radford et al., 2019 — \"Language Models are Unsupervised Multitask Learners\" (OpenAI)\n",
        "- Raffel et al., 2019 — \"Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer\" (arXiv:1910.10683)\n",
        "- Lewis et al., 2019 — \"BART: Denoising Sequence-to-Sequence Pre-training\" (arXiv:1910.13461)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Minimal demo: attention + encoder-only forward\n",
        "import torch\n",
        "from attention_mechanism import scaled_dot_product_attention\n",
        "from model_architectures import EncoderOnlyTransformer\n",
        "\n",
        "# Attention on toy tensors\n",
        "B, T, dk, dv = 2, 5, 8, 8\n",
        "Q = torch.randn(B, T, dk)\n",
        "K = torch.randn(B, T, dk)\n",
        "V = torch.randn(B, T, dv)\n",
        "attn_out, attn_w = scaled_dot_product_attention(Q, K, V)\n",
        "print('Attention output:', attn_out.shape, 'Weights:', attn_w.shape)\n",
        "\n",
        "# Tiny encoder-only forward\n",
        "vocab_size = 1000\n",
        "model = EncoderOnlyTransformer(vocab_size=vocab_size, d_model=64, num_heads=4, num_layers=2, d_ff=256, max_seq_len=32)\n",
        "input_ids = torch.randint(0, vocab_size, (B, T))\n",
        "enc = model(input_ids)\n",
        "print('Encoder-only output:', enc.shape)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
