{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Transformers: Comprehensive Implementation\n",
        "\n",
        "This notebook provides a complete implementation of text-based transformer architectures using the `rearrange` function for efficient tensor operations.\n",
        "\n",
        "## Learning Objectives\n",
        "- Implement attention mechanisms with rearrange operations\n",
        "- Build encoder-only, decoder-only, and encoder-decoder transformers\n",
        "- Understand multi-head attention and positional encoding\n",
        "- Practice with real tensor operations and shapes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Attention Mechanism Implementation\n",
        "\n",
        "We'll implement the core attention mechanisms using `rearrange` for efficient tensor operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
        "    \"\"\"\n",
        "    Scaled dot-product attention using rearrange operations.\n",
        "    \n",
        "    Args:\n",
        "        Q: Query tensor (B, H, N, D_k)\n",
        "        K: Key tensor (B, H, N, D_k)\n",
        "        V: Value tensor (B, H, N, D_v)\n",
        "        mask: Optional mask tensor (B, 1, N, N)\n",
        "    \n",
        "    Returns:\n",
        "        Attention output and attention weights\n",
        "    \"\"\"\n",
        "    B, H, N, D_k = Q.shape\n",
        "    \n",
        "    # Compute attention scores\n",
        "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(D_k)\n",
        "    \n",
        "    # Apply mask if provided\n",
        "    if mask is not None:\n",
        "        scores = scores.masked_fill(mask == 0, -1e9)\n",
        "    \n",
        "    # Apply softmax to get attention weights\n",
        "    attention_weights = F.softmax(scores, dim=-1)\n",
        "    \n",
        "    # Apply attention weights to values\n",
        "    output = torch.matmul(attention_weights, V)\n",
        "    \n",
        "    return output, attention_weights\n",
        "\n",
        "# Test the attention function\n",
        "B, H, N, D_k, D_v = 2, 4, 8, 64, 64\n",
        "# Parameter meanings:\n",
        "# - B: batch size (number of sequences processed in parallel)\n",
        "# - H: number of attention heads\n",
        "# - N: sequence length (number of tokens)\n",
        "# - D_k: per-head feature dimension for queries/keys\n",
        "# - D_v: per-head feature dimension for values\n",
        "# Typical: d_model = H * D_k and often D_k == D_v\n",
        "# Q, K, V expected shapes: (B, H, N, D_k/D_v)\n",
        "Q = torch.randn(B, H, N, D_k)\n",
        "K = torch.randn(B, H, N, D_k)\n",
        "V = torch.randn(B, H, N, D_v)\n",
        "\n",
        "output, weights = scaled_dot_product_attention(Q, K, V)\n",
        "print(f\"Attention output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Attention weights sum (should be 1): {weights.sum(dim=-1)[0, 0, :5]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Multi-Head Attention with Rearrange\n",
        "\n",
        "Implement multi-head attention using `rearrange` for efficient tensor reshaping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-head attention module using rearrange operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, dropout=0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.d_v = d_model // num_heads\n",
        "        \n",
        "        # Linear projections\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        B, N, D = query.shape\n",
        "        \n",
        "        # Linear projections and reshape to (B, N, H, D_k)\n",
        "        Q = self.W_q(query).view(B, N, self.num_heads, self.d_k)\n",
        "        K = self.W_k(key).view(B, N, self.num_heads, self.d_k)\n",
        "        V = self.W_v(value).view(B, N, self.num_heads, self.d_v)\n",
        "        \n",
        "        # Rearrange to (B, H, N, D_k) for attention computation\n",
        "        Q = rearrange(Q, 'B N H D -> B H N D')\n",
        "        K = rearrange(K, 'B N H D -> B H N D')\n",
        "        V = rearrange(V, 'B N H D -> B H N D')\n",
        "        \n",
        "        # Apply attention\n",
        "        attention_output, attention_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
        "        \n",
        "        # Rearrange back to (B, N, H, D_v)\n",
        "        attention_output = rearrange(attention_output, 'B H N D -> B N H D')\n",
        "        \n",
        "        # Concatenate heads and apply output projection\n",
        "        output = attention_output.contiguous().view(B, N, self.d_model)\n",
        "        output = self.W_o(output)\n",
        "        \n",
        "        return output, attention_weights\n",
        "\n",
        "# Test MultiHeadAttention\n",
        "d_model, num_heads = 512, 8\n",
        "mha = MultiHeadAttention(d_model, num_heads)\n",
        "\n",
        "B, N = 2, 10\n",
        "x = torch.randn(B, N, d_model)\n",
        "output, weights = mha(x, x, x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Attention weights shape: {weights.shape}\")\n",
        "print(f\"Multi-head attention working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Positional Encoding\n",
        "\n",
        "Implement different types of positional encoding using rearrange operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Sinusoidal positional encoding using rearrange operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        \n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n",
        "                           -(math.log(10000.0) / d_model))\n",
        "        \n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        \n",
        "        # Add batch dimension and rearrange for broadcasting\n",
        "        pe = rearrange(pe, 'N D -> 1 N D')\n",
        "        \n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # x: (B, N, D)\n",
        "        B, N, D = x.shape\n",
        "        \n",
        "        # Use rearrange to ensure proper broadcasting\n",
        "        pe = rearrange(self.pe[:, :N, :], '1 N D -> 1 N D')\n",
        "        \n",
        "        return x + pe\n",
        "\n",
        "class LearnedPositionalEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Learned positional embedding using rearrange operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.pos_embedding = nn.Embedding(max_len, d_model)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        B, N, D = x.shape\n",
        "        \n",
        "        # Create position indices\n",
        "        positions = torch.arange(N, device=x.device).unsqueeze(0)\n",
        "        positions = repeat(positions, '1 N -> B N', B=B)\n",
        "        \n",
        "        # Get positional embeddings\n",
        "        pos_emb = self.pos_embedding(positions)\n",
        "        \n",
        "        return x + pos_emb\n",
        "\n",
        "# Test positional encodings\n",
        "d_model, max_len = 128, 100\n",
        "sinusoidal_pe = SinusoidalPositionalEncoding(d_model, max_len)\n",
        "learned_pe = LearnedPositionalEmbedding(d_model, max_len)\n",
        "\n",
        "B, N = 2, 20\n",
        "x = torch.randn(B, N, d_model)\n",
        "\n",
        "x_with_sin_pe = sinusoidal_pe(x)\n",
        "x_with_learned_pe = learned_pe(x)\n",
        "\n",
        "print(f\"Original input shape: {x.shape}\")\n",
        "print(f\"With sinusoidal PE shape: {x_with_sin_pe.shape}\")\n",
        "print(f\"With learned PE shape: {x_with_learned_pe.shape}\")\n",
        "print(f\"Positional encodings working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Transformer Layer\n",
        "\n",
        "Implement a complete transformer layer with rearrange operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TransformerLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Complete transformer layer with rearrange operations.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.attention = MultiHeadAttention(d_model, num_heads, dropout)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "        \n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, mask=None):\n",
        "        # Self-attention with residual connection\n",
        "        attn_output, _ = self.attention(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        \n",
        "        # Feed-forward with residual connection\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        \n",
        "        return x\n",
        "\n",
        "# Test TransformerLayer\n",
        "d_model, num_heads, d_ff = 256, 8, 1024\n",
        "layer = TransformerLayer(d_model, num_heads, d_ff)\n",
        "\n",
        "B, N = 2, 15\n",
        "x = torch.randn(B, N, d_model)\n",
        "output = layer(x)\n",
        "\n",
        "print(f\"Input shape: {x.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Transformer layer working correctly!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Summary\n",
        "\n",
        "We've successfully implemented text transformer architectures using rearrange operations:\n",
        "\n",
        "### **Key Components:**\n",
        "1. **Attention Mechanisms**: Scaled dot-product attention\n",
        "2. **Multi-Head Attention**: Efficient implementation with rearrange\n",
        "3. **Positional Encoding**: Sinusoidal and learned embeddings\n",
        "4. **Transformer Layers**: Complete transformer blocks\n",
        "\n",
        "### **Benefits of Rearrange:**\n",
        "- Clean tensor reshaping for multi-head attention\n",
        "- Efficient positional encoding broadcasting\n",
        "- Intuitive tensor manipulation syntax\n",
        "\n",
        "### **Next Steps:**\n",
        "- Explore vision transformers\n",
        "- Implement decoder-only and encoder-decoder models\n",
        "- Add training loops and optimization"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
