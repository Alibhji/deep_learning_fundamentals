{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# From Pixel to Logit: Parameters, Memory, and MatMul on GPU\n",
        "\n",
        "This notebook dissects how `TinyViT32` loads parameters to GPU, how tensors are laid out, and how a single pixel flows through memory and registers to the final logit.\n",
        "\n",
        "## What you'll see\n",
        "- Exact device placement for each parameter\n",
        "- Raw device pointers (data_ptr) for tensors\n",
        "- A conceptual register/shared-memory diagram for GEMM\n",
        "- A step-by-step pixel path through patch → attention → MLP\n",
        "\n",
        "> Note: Pointers and physical addresses are for illustration. Frameworks abstract true HBM locations; kernels receive opaque device pointers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inspect parameters and device placement\n",
        "import torch\n",
        "from transformers07_embedded_vision.tiny_vit_embedded import TinyViT32  # adjust import if needed\n",
        "\n",
        "model = TinyViT32()\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "for n, p in model.named_parameters():\n",
        "    print(n, p.shape, p.dtype, p.device, hex(p.data_ptr()))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
