{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SOTA Optimization for Transformers: Training & Inference\n",
        "\n",
        "This notebook demonstrates modern optimization techniques for transformer models across training and inference, highlighting key ideas like FlashAttention, QLoRA, vLLM, and torch.compile.\n",
        "\n",
        "## Learning objectives\n",
        "- Apply mixed precision, checkpointing, and FSDP to reduce training cost\n",
        "- Use dynamic/PTQ/QAT quantization paths for inference efficiency\n",
        "- Understand K/V cache and paged attention patterns\n",
        "- Explore speculative decoding and multi-query attention benefits\n",
        "\n",
        "## References (selected)\n",
        "- FlashAttention v1/v2 — Dao et al., 2022/2023 (arXiv:2205.14135, 2307.08691)\n",
        "- QLoRA — Dettmers et al., 2023 (arXiv:2305.14314)\n",
        "- vLLM — Kwon et al., 2023 (Efficient Memory Management for LLM Serving)\n",
        "- GPTQ — Frantar et al., 2022 (arXiv:2210.17323)\n",
        "- AWQ — Lin et al., 2023 (arXiv:2306.00978)\n",
        "- ZeRO — Rajbhandari et al., 2020 (arXiv:1910.02054)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mixed precision + compile demo on a toy module\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from train_optimizations import MixedPrecisionTrainer\n",
        "from inference_optimizations import torch_compile_model\n",
        "\n",
        "class Tiny(nn.Module):\n",
        "    def __init__(self, d=256):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(d, 4*d), nn.GELU(), nn.Linear(4*d, d)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "model = Tiny()\n",
        "model = torch_compile_model(model)\n",
        "opt = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "trainer = MixedPrecisionTrainer(model, opt, use_bf16=False)\n",
        "\n",
        "x = torch.randn(8, 256)\n",
        "y = torch.randn(8, 256)\n",
        "\n",
        "def loss_fn(pred, tgt):\n",
        "    return nn.functional.mse_loss(pred, tgt)\n",
        "\n",
        "for _ in range(3):\n",
        "    pred = trainer.forward_with_autocast(model, x)\n",
        "    loss = loss_fn(pred, y)\n",
        "    val = trainer.step(loss)\n",
        "    print('Loss:', val)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quantization path: dynamic PTQ on a tiny head\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from quantization_toolkit import ptq_dynamic_linear_only\n",
        "\n",
        "head = nn.Sequential(nn.Linear(256, 256), nn.GELU(), nn.Linear(256, 128))\n",
        "qhead = ptq_dynamic_linear_only(head)\n",
        "print('Quantized module:', qhead)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
