{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e820baa",
   "metadata": {},
   "source": [
    "# Architecture Patterns: Connecting Encoders and Decoders\n",
    "\n",
    "This notebook covers common patterns for composing transformer systems: standard encoder–decoder, parameter sharing, hierarchical skip connections, and parallel processing. It also touches on scaling strategies (width/depth/compound), efficiency (distillation, quantization, pruning), and fine-tuning approaches (adapters, LoRA).\n",
    "\n",
    "## Learning objectives\n",
    "- Understand multiple ways to connect transformer components\n",
    "- Learn when to prefer each pattern\n",
    "- Try a small demo of cross-attention and shape manipulations with `einops`\n",
    "\n",
    "## Outline\n",
    "1. Standard encoder–decoder and cross-attention\n",
    "2. Shared-parameter encoder/decoder\n",
    "3. Hierarchical skip connections\n",
    "4. Parallel encoder/decoder updates\n",
    "5. Scaling strategies and practical tips\n",
    "6. Efficiency techniques and fine-tuning\n",
    "\n",
    "## References (Papers)\n",
    "- Vaswani et al., 2017 — \"Attention Is All You Need\" (arXiv:1706.03762)\n",
    "- Kaplan et al., 2020 — \"Scaling Laws for Neural Language Models\" (arXiv:2001.08361)\n",
    "- Tan & Le, 2019 — \"EfficientNet: Rethinking Model Scaling\" (arXiv:1905.11946)\n",
    "- Hinton et al., 2015 — \"Distilling the Knowledge in a Neural Network\" (arXiv:1503.02531)\n",
    "- Jacob et al., 2017 — \"Quantization and Training of Neural Networks\" (arXiv:1712.05877)\n",
    "- Frankle & Carbin, 2019 — \"The Lottery Ticket Hypothesis\" (arXiv:1803.03635)\n",
    "- Houlsby et al., 2019 — \"Parameter-Efficient Transfer Learning for NLP\" (arXiv:1902.00751)\n",
    "- Hu et al., 2021 — \"LoRA: Low-Rank Adaptation of Large Language Models\" (arXiv:2106.09685)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e3c357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-attention demo with shape ops\n",
    "import torch\n",
    "from einops import rearrange\n",
    "from connection_patterns import CrossAttention\n",
    "\n",
    "B, S, T, C = 2, 16, 12, 64  # batch, src_len, tgt_len, channels\n",
    "query = torch.randn(T, B, C)   # nn.MultiheadAttention expects (seq, batch, embed)\n",
    "key   = torch.randn(S, B, C)\n",
    "value = torch.randn(S, B, C)\n",
    "\n",
    "x = query\n",
    "attn = CrossAttention(d_model=C, num_heads=4)\n",
    "output, weights = attn(x, key, value)\n",
    "print('Cross-attn out:', output.shape, 'weights:', weights.shape)\n",
    "\n",
    "# Example of requested rearrange pattern\n",
    "# Suppose we flattened (B*N, C) and want to restore (B, N, C)\n",
    "B_, N, C_ = 2, 8, 64\n",
    "flat = torch.randn(B_*N, C_)\n",
    "restored = rearrange(flat, '(B N) C -> B N C', B=B_, N=N)\n",
    "print('Restored shape:', restored.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
