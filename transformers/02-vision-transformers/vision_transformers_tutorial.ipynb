{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformers (ViT, DeiT, Swin, DETR, SegFormer)\n",
        "\n",
        "This notebook covers transformer-based architectures for vision, from patch embeddings and ViT to hierarchical Swin, detection with DETR, and segmentation with SegFormer.\n",
        "\n",
        "## Learning objectives\n",
        "- Understand image-to-token conversion via patch embeddings\n",
        "- Learn ViT basics and hierarchical extensions (Swin)\n",
        "- See how attention powers detection/segmentation (DETR/SegFormer)\n",
        "- Use einops `rearrange` to reshape tensors cleanly\n",
        "\n",
        "## References (Papers)\n",
        "- Dosovitskiy et al., 2020 — \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (arXiv:2010.11929)\n",
        "- Touvron et al., 2020 — \"Training data-efficient image transformers & distillation through attention\" (arXiv:2012.12877)\n",
        "- Liu et al., 2021 — \"Swin Transformer\" (arXiv:2103.14030)\n",
        "- Carion et al., 2020 — \"End-to-End Object Detection with Transformers (DETR)\" (arXiv:2005.12872)\n",
        "- Xie et al., 2021 — \"SegFormer\" (arXiv:2105.15203)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patch embedding + einops rearrange demo\n",
        "import torch\n",
        "from einops import rearrange\n",
        "from patch_embedding import PatchEmbedding, restore_batch_from_flat_embeddings\n",
        "\n",
        "B, C, H, W = 2, 3, 224, 224\n",
        "x = torch.randn(B, C, H, W)\n",
        "pe = PatchEmbedding(img_size=224, patch_size=16, in_channels=3, embed_dim=64)\n",
        "emb = pe(x)  # (B, N, C)\n",
        "print('Patch embeddings:', emb.shape)\n",
        "\n",
        "# Flatten (B, N, C) -> (B*N, C) and restore with rearrange\n",
        "B_, N, C_ = emb.shape\n",
        "flat = rearrange(emb, 'B N C -> (B N) C')\n",
        "restored = restore_batch_from_flat_embeddings(flat, B_, N)\n",
        "print('Flat:', flat.shape, 'Restored:', restored.shape)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
