{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c03ec3f",
   "metadata": {},
   "source": [
    "# Advanced Concepts: Attention Variants, Positional Biases, Norms, and Optimizers\n",
    "\n",
    "This notebook surveys advanced techniques: linear/sparse/local attention, rotary and ALiBi positional methods, normalization beyond LayerNorm (RMSNorm), and recent optimizers (Lion, Sophia). Includes a small runnable comparison snippet.\n",
    "\n",
    "## Learning objectives\n",
    "- Know when to use alternative attention mechanisms\n",
    "- Understand modern positional encodings and their benefits\n",
    "- Try practical snippets to compare behaviors\n",
    "\n",
    "## Outline\n",
    "1. Linear vs standard attention\n",
    "2. Local and sparse attention\n",
    "3. Rotary (RoPE) and ALiBi\n",
    "4. RMSNorm vs LayerNorm\n",
    "5. Lion and Sophia optimizers\n",
    "\n",
    "## References (Papers)\n",
    "- Tay et al., 2020 — \"Efficient Transformers: A Survey\" (arXiv:2009.06732)\n",
    "- Su et al., 2021 — \"RoFormer: Rotary Position Embedding\" (arXiv:2104.09864)\n",
    "- Press et al., 2021 — \"ALiBi: Train Short, Test Long\" (arXiv:2108.12409)\n",
    "- Dao et al., 2022 — \"FlashAttention\" (arXiv:2205.14135)\n",
    "- Chen et al., 2023 — \"Lion Optimizer\" (arXiv:2302.06675)\n",
    "- Liu et al., 2023 — \"Sophia Optimizer\" (arXiv:2305.14342)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37446a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare standard vs linear attention on toy data\n",
    "import torch\n",
    "from attention_variants import LinearAttention\n",
    "\n",
    "B, T, C = 2, 64, 128\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "# Standard multihead (for reference): expects (seq, batch, embed)\n",
    "std_attn = torch.nn.MultiheadAttention(embed_dim=C, num_heads=4)\n",
    "std_out, _ = std_attn(x.transpose(0,1), x.transpose(0,1), x.transpose(0,1))\n",
    "std_out = std_out.transpose(0,1)\n",
    "print('Standard attention out:', std_out.shape)\n",
    "\n",
    "# Linear attention\n",
    "lin_attn = LinearAttention(d_model=C, num_heads=4)\n",
    "lin_out = lin_attn(x)\n",
    "print('Linear attention out:', lin_out.shape)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
